{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXQRtpkxNHWXPq0FlugEBC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshita23sharma/opensource_llms/blob/main/QNA_on_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKZELqdDPjxQ"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.1.2\n",
        "!pip -q install langchain\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "# !pip -q install datasets\n",
        "# loralib\n",
        "# sentencepiece\n",
        "!pip -q install pypdf\n",
        "\n",
        "!pip -q install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "id": "kbRtfOu2VIoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader #load pdf and extract data\n",
        "from langchain.document_loaders import TextLoader #upload csv and extract data from it\n",
        "from langchain.document_loaders import Docx2txtLoader #upload docx and extract data from it\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "C0D6u6e_VKAC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs1Pv2rSasD5",
        "outputId": "63d48640-8ced-4c89-e824-c4772a1f475d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Load the Documents and Extract Text From Them\n"
      ],
      "metadata": {
        "id": "YhYyNmTbWIjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = \"/content/drive/MyDrive/projects/qna_llama2\""
      ],
      "metadata": {
        "id": "B7JlRrWcWN-k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import Docx2txtLoader\n"
      ],
      "metadata": {
        "id": "TrbQYsZsdboO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document=[]\n",
        "for file in os.listdir(docs):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    pdf_path=docs+ \"/\" + file\n",
        "    loader=PyPDFLoader(pdf_path)\n",
        "    document.extend(loader.load())\n",
        "  # elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "  #   doc_path=docs+ \"/\" +file\n",
        "  #   loader=Docx2txtLoader(doc_path)\n",
        "  #   document.extend(loader.load())\n",
        "  # elif file.endswith('.txt'):\n",
        "  #   text_path=docs + \"/\" +file\n",
        "  #   loader=TextLoader(text_path)\n",
        "  #   document.extend(loader.load())"
      ],
      "metadata": {
        "id": "apjgc5nrV0fo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Split the Document into Chunks\n"
      ],
      "metadata": {
        "id": "WFC1QZCBXWsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_splitter=CharacterTextSplitter(separator='\\n', chunk_size=500, chunk_overlap=100)\n"
      ],
      "metadata": {
        "id": "0V357SsuWjGe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks=document_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "AGC2ctd8Xbju"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Download the Embeddings from Hugging Face, Download the Sentence Transformer Embeddings"
      ],
      "metadata": {
        "id": "TR0rRPhYXfp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "ijQSbSJxXcbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Setting Up Chroma as our Vector Database\n"
      ],
      "metadata": {
        "id": "mFaTY6v0Xljz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n"
      ],
      "metadata": {
        "id": "uMhh1exKXnSe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist()\n"
      ],
      "metadata": {
        "id": "tNYgaji4Xocq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Login into Hugging Face Account to Download the Model\n"
      ],
      "metadata": {
        "id": "U6kmPtN_XqKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "oaeAg_fsXq1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Download the Llama 2 7B Chat Model\n"
      ],
      "metadata": {
        "id": "1K0NHI8AX6Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True,)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                              #load_in_8bit=True,\n",
        "                                              load_in_4bit=True\n",
        "                                             )\n",
        ""
      ],
      "metadata": {
        "id": "sXD0An6jXvfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Creating a Hugging Face Pipeline\n"
      ],
      "metadata": {
        "id": "peuw4Hv2X8pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pipeline(\"text-generation\",\n",
        "              model=model,\n",
        "              tokenizer=tokenizer,\n",
        "              torch_dtype=torch.bfloat16,\n",
        "              device_map='auto',\n",
        "              max_new_tokens=512,\n",
        "              min_new_tokens=-1,\n",
        "              top_k=30\n",
        "\n",
        "              )"
      ],
      "metadata": {
        "id": "G6E-bDZXX06x"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
      ],
      "metadata": {
        "id": "3i4hYW0UX_S0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Creating a memory object which is necessary to track inputs/outputs and hold a conversation"
      ],
      "metadata": {
        "id": "7pm-PagoYDF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
      ],
      "metadata": {
        "id": "AIxjBd6iYEkI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create our Q/A Chain\n",
        "pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                             retriever=vectordb.as_retriever(search_kwargs={'k':6}),\n",
        "                                             verbose=False, memory=memory)\n",
        ""
      ],
      "metadata": {
        "id": "iQ8uiw5NYGRg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=pdf_qa({\"question\":\"llama2 is trained on which dataset\"})\n"
      ],
      "metadata": {
        "id": "L4O3ILtbd4MP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9BGcz2OelvJ",
        "outputId": "16098f80-51e5-4685-85fa-a02b26de8780"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'llama2 is trained on which dataset',\n",
              " 'chat_history': [HumanMessage(content='llama2 is trained on which dataset'),\n",
              "  AIMessage(content=' Based on the text, Llama 2 is trained on a new mix of data from publicly available sources.\\n\\nNote: The text does not explicitly state the name of the dataset used for pretraining, but it does mention that the data is from publicly available sources. Therefore, the answer is based on the information provided in the text and the context of the passage.')],\n",
              " 'answer': ' Based on the text, Llama 2 is trained on a new mix of data from publicly available sources.\\n\\nNote: The text does not explicitly state the name of the dataset used for pretraining, but it does mention that the data is from publicly available sources. Therefore, the answer is based on the information provided in the text and the context of the passage.'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}